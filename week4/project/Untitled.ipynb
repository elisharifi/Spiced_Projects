{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcae39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser = argparse.ArgumentParser(description=\"A program to predict the artist from a piece of text(word). Singers are Ed Sheeran and Taylor Swift. \") # initialization\n",
    "args = parser.parse_args()\n",
    "ps = PorterStemmer()\n",
    "\n",
    "## Function for downloading the webpage of a singer\n",
    "\n",
    "def download_web(web_page):\n",
    "    return wbpg = requests.get(web_page)\n",
    "\n",
    "## Function for downloading the lyrics of a singer\n",
    "\n",
    "def lyric_down(path,class_list, n = 101):\n",
    "    \n",
    "    \"\"\"This function download the lyrics of a singer from the given class \n",
    "    and saves it into the given path\"\"\"\n",
    "    \n",
    "    for index , element in enumerate(class_list):\n",
    "        if index < n:\n",
    "            line = element.find_all('a')[0]\n",
    "            h_link = re.findall(pattern ='<a href=\"(.+)\">(.+)</a>', string = str(line))\n",
    "            link = 'https://www.lyrics.com' + h_link[0][0]\n",
    "            song = requests.get(link)\n",
    "            song_html = song.text\n",
    "\n",
    "            lyric_soup = BeautifulSoup(song_html, 'html.parser')\n",
    "            lyric_text = lyric_soup.find_all('pre', attrs = {'class': \"lyric-body\"})\n",
    "\n",
    "            file_name = path + 'song_num_' + str(index) + '.txt'\n",
    "            file = open(file_name, 'w')\n",
    "            file.write(lyric_text[0].text)\n",
    "            file.close()\n",
    "            \n",
    "## Function for creating corpus for each singer            \n",
    "\n",
    "def corpus(path, n= 101):\n",
    "    \"\"\"This function reads each text file (in total 101 files), cleans it by removing \n",
    "    numbers, new lines and punctuations except apostrophe. Then it will stemm the words \n",
    "    and finally creats a corpus of cleaned stemmed lyrics. \"\"\"\n",
    "    \n",
    "    total_stemmed = []\n",
    "    for i in range (n):\n",
    "        stemmed_text = []\n",
    "        file_name = path + 'song_num_' + str(i) + '.txt'\n",
    "        file = open(file_name, \"r\")\n",
    "        text = file.read()\n",
    "\n",
    "        text_clean1 = re.sub(r\"[^\\w\\s']\", '', text)      #rmv punctuations exc apostrophe \n",
    "        text_clean2= re.sub(r'[\\n]', ' ', text_clean1)   #rmv new lines\n",
    "        text_clean3 = re.sub(r'[0-9]+', '', text_clean2)  #rmv numbers\n",
    "\n",
    "        words = word_tokenize(text_clean3)                #spliting each song into words\n",
    "        for word in words:                                # stemming words\n",
    "            new_word = ps.stem(word)                 \n",
    "            stemmed_text.append(new_word)           #appending stemmed words to a list\n",
    "#             print(stemmed_text)\n",
    "\n",
    "        total_stemmed.append(stemmed_text)       #appending stemmed songs to a list\n",
    "#         print(total_stemmed)\n",
    "        file.close()\n",
    "\n",
    "\n",
    "    corpus=[0]*n                                #combining stemmed words of each song\n",
    "    for index, song in enumerate(total_stemmed):   #creating corpus of songs (cleaned words)\n",
    "        corpus[index] = ' '.join(total_stemmed[index])    \n",
    "\n",
    "    return corpus\n",
    "\n",
    "\n",
    "            \n",
    "#positional arguments:\n",
    "parser.add_argument('web_page1', help= \"The webpage address of a singer that has hyperlinks of her/his songs\")\n",
    "parser.add_argument('web_page2', help= \"The webpage address of the second singer that has hyperlinks of her/his songs\")\n",
    "parser.add_argument('lyric_path1', help= \"The address of the folder including the text file of the first singer songs\")\n",
    "parser.add_argument('lyric_path2', help= \"The address of the folder including the text file of the second singer songs\")\n",
    "\n",
    "\n",
    "#downloading the page for first singer\n",
    "\n",
    "user_web_page1 = args.web_page1\n",
    "user_web1 = download_web(user_web_page1)\n",
    "\n",
    "##BeautifulSoup object\n",
    "\n",
    "web_soup = BeautifulSoup(user_web1.text, 'html.parser')\n",
    "class_list_ed = web_soup.find_all('td', attrs = {'class': \"tal qx\"})\n",
    "\n",
    "#downloading the page for second singer\n",
    "\n",
    "user_web_page2 = args.web_page2\n",
    "user_web2 = download_web(user_web_page2)\n",
    "\n",
    "#BeautifulSoup object for second singer\n",
    "\n",
    "ts_html = user_web2.text\n",
    "ts_soup = BeautifulSoup(ts_html, 'html.parser')\n",
    "class_list_ts = ts_soup.find_all('td', attrs = {'class': \"tal qx\"})\n",
    "\n",
    "# Creating corpus for first singer\n",
    "\n",
    "user_lyric_path1 = args.lyric_path1\n",
    "corpus_1 = corpus(user_lyric_path1)\n",
    "\n",
    "# Creating corpus for second singer\n",
    "\n",
    "user_lyric_path2 = args.lyric_path2\n",
    "corpus_2 = corpus(user_lyric_path2)\n",
    "\n",
    "# Creating the matrix (vectorizing corpus) and then Normalizing using TF-IDF and finally creating a data frame for each artist\n",
    "\n",
    "#first singer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "matrix_1 = vectorizer.fit_transform(corpus_1)\n",
    "\n",
    "tf = TfidfTransformer()\n",
    "transformed_1 = tf.fit_transform(matrix_1)\n",
    "tdf_1 = pd.DataFrame(transformed_1.todense(), columns=vectorizer.get_feature_names_out())\n",
    "tdf_1['artist'] = 'first singer'\n",
    "\n",
    "#second singer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "matrix_2 = vectorizer.fit_transform(corpus_2)\n",
    "\n",
    "tf = TfidfTransformer()\n",
    "transformed_2 = tf.fit_transform(matrix_2)\n",
    "tdf_2 = pd.DataFrame(transformed_2.todense(), columns=vectorizer.get_feature_names_out())\n",
    "tdf_2['artist'] = 'second singer'\n",
    "\n",
    "## Stacking the data frames of two artists and filling NaNs with zero\n",
    "\n",
    "df = pd.concat([tdf_1,tdf_2])\n",
    "df = df.fillna(0)\n",
    "\n",
    "### Fitting a  classification model (LogisticRegression)\n",
    "\n",
    "X = df.drop(['artist'], axis = 1)\n",
    "y = df['artist']\n",
    "\n",
    "df.reset_index()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=40)\n",
    "\n",
    "m_lr = LogisticRegression()\n",
    "m_lr.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "lr_tr = m_lr.score(x_train, y_train)\n",
    "lr_ts = m_lr.score(x_test, y_test)\n",
    "\n",
    "print(f\"\"\" LogisticRegression score for train data: {lr_tr}\\n LogisticRegression score for test data: {lr_ts}\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
